
<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <title>2D To 3D Reconstruction</title>
    <link href="images/favicon-bb70b6e5.ico" rel="icon" type="image/ico" />
    <link href="https://fonts.googleapis.com/css?family=Raleway:900italic" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="stylesheets/header-a3ad35e9.css">



    <style>
      .highlight table td { padding: 5px; }
.highlight table pre { margin: 0; }
.highlight .gh {
  color: #999999;
}
.highlight .sr {
  color: #f6aa11;
}
.highlight .go {
  color: #888888;
}
.highlight .gp {
  color: #555555;
}
.highlight .gs {
}
.highlight .gu {
  color: #aaaaaa;
}
.highlight .nb {
  color: #f6aa11;
}
.highlight .cm {
  color: #75715e;
}
.highlight .cp {
  color: #75715e;
}
.highlight .c1 {
  color: #75715e;
}
.highlight .cs {
  color: #75715e;
}
.highlight .c, .highlight .ch, .highlight .cd, .highlight .cpf {
  color: #75715e;
}
.highlight .err {
  color: #960050;
}
.highlight .gr {
  color: #960050;
}
.highlight .gt {
  color: #960050;
}
.highlight .gd {
  color: #49483e;
}
.highlight .gi {
  color: #49483e;
}
.highlight .ge {
  color: #49483e;
}
.highlight .kc {
  color: #66d9ef;
}
.highlight .kd {
  color: #66d9ef;
}
.highlight .kr {
  color: #66d9ef;
}
.highlight .no {
  color: #66d9ef;
}
.highlight .kt {
  color: #66d9ef;
}
.highlight .mf {
  color: #ae81ff;
}
.highlight .mh {
  color: #ae81ff;
}
.highlight .il {
  color: #ae81ff;
}
.highlight .mi {
  color: #ae81ff;
}
.highlight .mo {
  color: #ae81ff;
}
.highlight .m, .highlight .mb, .highlight .mx {
  color: #ae81ff;
}
.highlight .sc {
  color: #ae81ff;
}
.highlight .se {
  color: #ae81ff;
}
.highlight .ss {
  color: #ae81ff;
}
.highlight .sd {
  color: #e6db74;
}
.highlight .s2 {
  color: #e6db74;
}
.highlight .sb {
  color: #e6db74;
}
.highlight .sh {
  color: #e6db74;
}
.highlight .si {
  color: #e6db74;
}
.highlight .sx {
  color: #e6db74;
}
.highlight .s1 {
  color: #e6db74;
}
.highlight .s, .highlight .sa, .highlight .dl {
  color: #e6db74;
}
.highlight .na {
  color: #a6e22e;
}
.highlight .nc {
  color: #a6e22e;
}
.highlight .nd {
  color: #a6e22e;
}
.highlight .ne {
  color: #a6e22e;
}
.highlight .nf, .highlight .fm {
  color: #a6e22e;
}
.highlight .vc {
  color: #ffffff;
  background-color: #272822;
}
.highlight .nn {
  color: #ffffff;
  background-color: #272822;
}
.highlight .nl {
  color: #ffffff;
  background-color: #272822;
}
.highlight .ni {
  color: #ffffff;
  background-color: #272822;
}
.highlight .bp {
  color: #ffffff;
  background-color: #272822;
}
.highlight .vg {
  color: #ffffff;
  background-color: #272822;
}
.highlight .vi {
  color: #ffffff;
  background-color: #272822;
}
.highlight .nv, .highlight .vm {
  color: #ffffff;
  background-color: #272822;
}
.highlight .w {
  color: #ffffff;
  background-color: #272822;
}
.highlight {
  color: #ffffff;
  background-color: #272822;
}
.highlight .n, .highlight .py, .highlight .nx {
  color: #ffffff;
  background-color: #272822;
}
.highlight .ow {
  color: #f92672;
}
.highlight .nt {
  color: #f92672;
}
.highlight .k, .highlight .kv {
  color: #f92672;
}
.highlight .kn {
  color: #f92672;
}
.highlight .kp {
  color: #f92672;
}
.highlight .o {
  color: #f92672;
}
    </style>
    <link href="stylesheets/screen-9469d2b6.css" rel="stylesheet" media="screen" />
    <link href="stylesheets/print-c427a123.css" rel="stylesheet" media="print" />
      <script src="javascripts/all-b12a2749.js"></script>
  </head>

  <body class="index" data-languages="[&quot;matlab&quot;]">
    <header></header>

    <div class="slate-wrapper"> 
      <a href="#" id="nav-button">
        <span>
          NAV
          <img src="images/navbar-cad8cdcb.png" alt="" />
        </span>
      </a>
      <div class="toc-wrapper">
        <img src="images/logo-255b6f0d.png" class="logo" alt="" />
          <div class="lang-selector">
                <a href="#" data-language-name="matlab">---</a>
          </div>
          <div class="search">
            <input type="text" class="search" id="input-search" placeholder="Search">
          </div>
          <ul class="search-results"></ul>
        <ul id="toc" class="toc-list-h1">
            <li>
              <a href="#2d-to-3d-object-reconstruction" class="toc-h1 toc-link" data-title="2D To 3D Object Reconstruction">2D To 3D Object Reconstruction</a>
            </li>
            <li>
              <a href="#background" class="toc-h1 toc-link" data-title="Background">Background</a>
                <ul class="toc-list-h2">
                    <li>
                      <a href="#goal" class="toc-h2 toc-link" data-title="Goal">Goal</a>
                    </li>
                    <li>
                      <a href="#motivation" class="toc-h2 toc-link" data-title="Motivation">Motivation</a>
                    </li>
                    <li>
                      <a href="#current-state-of-the-art" class="toc-h2 toc-link" data-title="Current State of the Art">Current State of the Art</a>
                    </li>
                </ul>
            </li>
            <li>
              <a href="#our-solution" class="toc-h1 toc-link" data-title="Our Solution">Our Solution</a>
                <ul class="toc-list-h2">
                    <li>
                      <a href="#image-alignment" class="toc-h2 toc-link" data-title="Image Alignment">Image Alignment</a>
                    </li>
                    <li>
                      <a href="#light-direction-estimation" class="toc-h2 toc-link" data-title="Light Direction Estimation">Light Direction Estimation</a>
                    </li>
                    <li>
                      <a href="#photometric-stereo" class="toc-h2 toc-link" data-title="Photometric Stereo">Photometric Stereo</a>
                    </li>
                    <li>
                      <a href="#clustering-normals" class="toc-h2 toc-link" data-title="Clustering Normals">Clustering Normals</a>
                    </li>
                    <li>
                      <a href="#normal-to-depth-map" class="toc-h2 toc-link" data-title="Normal to depth map">Normal to depth map</a>
                    </li>
                    <li>
                      <a href="#depth-map-to-stl" class="toc-h2 toc-link" data-title="Depth Map to STL">Depth Map to STL</a>
                    </li>
                </ul>
            </li>
            <li>
              <a href="#result" class="toc-h1 toc-link" data-title="Result">Result</a>
            </li>
            <li>
              <a href="#future-development-and-applications" class="toc-h1 toc-link" data-title="Future Development and Applications">Future Development and Applications</a>
            </li>
            <li>
              <a href="#source-code-and-dataset" class="toc-h1 toc-link" data-title="Source Code and Dataset">Source Code and Dataset</a>
                <ul class="toc-list-h2">
                    <li>
                      <a href="#brief-details" class="toc-h2 toc-link" data-title="Brief Details">Brief Details</a>
                    </li>
                    <li>
                      <a href="#codebase-breakdown" class="toc-h2 toc-link" data-title="CodeBase Breakdown">CodeBase Breakdown</a>
                    </li>
                </ul>
            </li>
        </ul>
          <ul class="toc-footer">
              <li>Developed For <a href="https://pages.cs.wisc.edu/~mohitg/courses/CS639/index.html">CS639</a></li>
              <li>Maintained By <a href="https://github.com/Alphamineron">Ankur Singh</a></li>
          </ul>
      </div>
      <div class="page-wrapper">
        <div class="dark-box"></div>
        <div class="content">
          <!-- [CHAPTER1](Chapter1.html) -->
<h1 id='2d-to-3d-object-reconstruction'>2D To 3D Object Reconstruction</h1>
<p><strong>Project By</strong> <a href="https://github.com/Alphamineron">Ankur Singh</a>, <a href="https://github.com/kjoseph8">Kevin Joseph</a>, <a href="https://github.com/sakowal">Sarah Kowal</a>, <a href="https://github.com/Rohit-Singhal4">Rohit Singhal</a></p>
<h1 id='background'>Background</h1><h2 id='goal'>Goal</h2>
<blockquote>
<iframe width="560" height="315" src="https://www.youtube.com/embed/SF3HfCjwg3s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</blockquote>

<p>The idea for this project was created through an interest  in game design and development. A curiosity in the methodology and process of creating huge virtual worlds, our group thought of researching how objects are created to look so realistic and proportionate. This led us to the central objective of this project which is to use computer vision to convert real world images of objects into mesh-based 3D models that could be imported into graphics projects or video games.</p>

<aside class="warning">
The source code and dataset along with instructions to explore the project on your own machine are included near the end of this page!
</aside>
<h2 id='motivation'>Motivation</h2>
<p>2D to 3D reconstruction has a vast number of applications from game design to modeling. The process of recreating an object in three dimensions can be highly time consuming and require a lot of hardware power. The time it takes to create 3D models for an environment often causes developers to create more minimalistic environments with fewer 3D models, often sacrificing how realistic the environment looks. By creating a program that could convert objects in images into 3D models, it might be possible to quickly create multiple 3D models in just a fraction of the time it would normally take. An extension of this project could even involve processing an entire environment from images rather than just individual objects. This problem is applicable in a large variety of fields. It can be applied to game development, real estate, and any other industry that could benefit from having a 3 dimensional model of a physical object. A program like this can increase efficiency and provide more detailed and realistic visuals for images of real world objects.</p>

<p>Using computer vision, the time and effort required to create such models can be reduced drastically. The implementation of such a program in this project focuses on reconstructing 2D models to 3 Dimensions and attempts to expand the software to allow the user to specify how high/low poly they want the meshes to be and to make the software easier for users to use.</p>
<h2 id='current-state-of-the-art'>Current State of the Art</h2>
<p>There are many types of photogrammetry software and equipment that have the ability to turn images into 3D models however, they all have their limitations. Free software that is available is online. These sites are good at creating models from images that are asymmetrical. Other sites can create 3D models from a single asymmetrical image using artificial intelligence. The biggest caveat to this type of software is that it is often centralized around one type of object, for example, facial reconstruction. More advanced software can produce more realistic models using multiple photos. Most photogrammetry software takes information from 2D images and creates a point cloud which becomes a 3D mesh. Most preexisting photogrammetry software default to producing high poly 3D meshes, which would be out of place in an environment with premade low poly assets. </p>

<p>The current state-of-the-art for photometric stereo, specifically, involves maintaining a consistent camera position, and inputting the light source directions of each image. These steps could be difficult for regular users.</p>

<p>Allowing the user to determine and express their preferences for low vs high poly models while also using uncalibrated photometric stereo in this implementation provides the user with higher flexibility for the output 3D model.</p>
<h1 id='our-solution'>Our Solution</h1><h2 id='image-alignment'>Image Alignment</h2>
<p>Multiple input images are typically required in 2D to 3D object reconstruction. When using multiple images of one object, it is important that the same points on the object can be identified in each picture. Our initial approach to ensure that the photos were aligned properly was using a SIFT implementation with a homographic projection. SIFT would be used to locate key points in the images and the homographic projection only keeps the parts of the images that intersect. This implementation would make it easier for the user as they would not have to be too concerned with the camera position. Unfortunately, the SIFT implementation failed due to strongly differing lighting conditions in the dataset. Our solution to this problem was to require a stationary camera position. </p>
<h2 id='light-direction-estimation'>Light Direction Estimation</h2>
<p>We then needed to address finding the light directions of each image. Our light source direction vector estimations for the images were obtained using a Convolutional Neural Network. Prior to the CNN, the light source directions were changed from cartesian to spherical coordinates, and were later transformed back. We found that when a large number of photos is used, the light direction estimates produced by the CNN increased the photometric stereo accuracy. </p>
<h2 id='photometric-stereo'>Photometric Stereo</h2>
<p>The next step was identifying varying depths and surface orientations in the images. We used a photometric stereo to produce surface normals at each pixel of each image. We solved for the Lambertian case, which only cares about diffuse lighting, by doing the matrix calculation mentioned in the lecture with the image intensities and estimated light source directions. To allow for the use of more than 3 images to reduce the effects of noise, we used least squares estimation.</p>
<h2 id='clustering-normals'>Clustering Normals</h2>
<p>The next step helps with the defining characteristic of our program, making low-poly models. The photometric stereo is very helpful because it produces a normal map instead of just a depth map. With the normal map, we can use the image coordinates and surface normals in a clustering algorithm that groups nearby pixels with similar surface normals. Using k-means clustering is what gives the user the ability to choose their poly level. A higher value of k yields a higher-poly model because there will be more clusters with different surface normals at their centers.</p>
<h2 id='normal-to-depth-map'>Normal to depth map</h2>
<p>Our next concern was generating a depth map from the normal map. The normals can be thought of as the derivative of the depth. Since the normal map is discrete, our initial thought was to iteratively go through the pixels and add the normal’s respective dx/dz or dy/dz values. However, this creates a lot of noise. To compensate for this, we instead decided to use the frankot-chellappa algorithm. This algorithm involves a least squares estimation over the fourier domain to make a depth map with less noise.</p>
<h2 id='depth-map-to-stl'>Depth Map to STL</h2>
<p>We used the MatLab surf2stl function to convert our depth maps into STL files that can be used in other programs. The clustering makes the STL file appear low-poly however, all files have the same level of poly. This could still be useful to have the model fit into a pre-existing low-poly environment, but it would not help with performance unless the user uses a tool like Blender to combine faces of the mesh.</p>
<h1 id='result'>Result</h1>
<blockquote>
<table><thead>
<tr>
<th>Bear (20 Angles)</th>
<th>Buddha (20 Angles)</th>
</tr>
</thead><tbody>
<tr>
<td><img src="images/bear_model-64537374.gif" alt="3D Bear gif" /></td>
<td><img src="images/budda_model-a7a93c46.gif" alt="3D Buddha gif" /></td>
</tr>
</tbody></table>
</blockquote>

<p>Our computer vision model is able to successfully create stl files for a specific viewpoint/angle of a 3D model (corresponding to the 2D image data). We did realize certain strengths and limitations of our approach, such as:</p>

<ul>
<li>Our program is versatile enough to create accurate 3D models from the DiLiGenT-Test and DiLiGenT-MV datasets, even though the CNN was only trained on the DiLiGenT dataset with a fixed viewing angle.</li>
<li>However, our light source estimation is not as accurate on real-world images, but that’s expected given we don’t have enough data in our dataset for more diverse environments and light sources.</li>
</ul>

<table><thead>
<tr>
<th>Object Name</th>
<th>2D Image</th>
<th>3D Render</th>
</tr>
</thead><tbody>
<tr>
<td>Bear</td>
<td><img src="images/bear-d1c38918.png" alt="2D Bear" /></td>
<td><img src="images/bear_model-32e6f155.png" alt="3D Bear" /></td>
</tr>
<tr>
<td>Buddha</td>
<td><img src="images/Buddha-1fe58552.png" alt="2D Buddha" /></td>
<td><img src="images/buddha_model-a342baee.png" alt="3D Buddha" /></td>
</tr>
</tbody></table>
<h1 id='future-development-and-applications'>Future Development and Applications</h1>
<p>There are a variety of aspects of this implementation that can be improved in the future. Currently, the implementation requires the user to have a still camera as it uses uncalibrated photometric stereo. Improving this aspect by using light source estimation techniques can provide added benefit to the user as they would have to pay attention to the placement of the camera less, creating a more flexible program. Another area of improvement could be to account for non-lambertian materials, but that would most likely require using a CNN to create the entire normal map instead of just predicting light source directions.</p>

<p>Furthermore, the convolutional neural network could be improved by training more diverse data or by using transfer learning to help our data adapt to different environments and light sources. Additionally, it would be good to find a way to change the stl creation from using surftostl to stlwrite using MatLab’s triangulation objects to actually decrease the poly count instead of just looking low-poly.</p>

<p>These programs can be implemented in any field or area that requires virtual modeling. This includes game design, real estate, medicine, film, etc. It decreases the time and effort needed to construct objects in virtually three dimensions.</p>
<h1 id='source-code-and-dataset'>Source Code and Dataset</h1><h2 id='brief-details'>Brief Details</h2>
<blockquote>
<p><img src="images/dilgent_dataset-98af927c.png" alt="Dataset Preview Image" /></p>
</blockquote>

<p>Here, we are usng the <a href="https://sites.google.com/site/photometricstereodata/single">‘DiLiGenT’ Photometric Stereo Dataset</a>.</p>

<blockquote>
<p><em>Boxin Shi, Zhipeng Mo, Zhe Wu, Dinglong Duan, Sai-Kit Yeung, and Ping Tan, &quot;A Benchmark Dataset and Evaluation for Non-Lambertian and Uncalibrated Photometric Stereo&quot;, In IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), Volume 41, Issue 2, Pages 271-284,2019. <a href="http://alumni.media.mit.edu/~shiboxin/files/Shi_TPAMI19.pdf">PDF</a> <a href="https://drive.google.com/file/d/1cf3uYTTTfS5ZyCvj2agdmVsmtAcTZpX2/view">Supplementary Material</a></em></p>
</blockquote>

<p>We modified the DiLiGenT dataset to be smaller and easier to load the data. Download the dataset from <a href="https://drive.google.com/file/d/1qqTqhjb5wVYreaZFCB5s-_SVZgIHdYXv/view?usp=sharing">this Google Drive</a>.</p>

<p><code>light_cnn_inceptionv3.mat</code>: Our code includes a pre-trained network for estimating light-source directions based on the inception-v3 CNN in MatLab.</p>

<p>Our code includes many different functions for different steps of our pipeline. You can find the code on <a href="https://github.com/Alphamineron/2D-To-3D-Object-Reconstruction/tree/master">Github</a>.</p>
<h2 id='codebase-breakdown'>CodeBase Breakdown</h2>
<blockquote>
<p><code>git clone git@github.com:Alphamineron/2D-To-3D-Object-Reconstruction.git</code></p>

<p>For further steps, you need a valid matlab licensed binary installed on your system. Below are example commands for executing matlab on a macOS machine.</p>

<p><strong>Running Matlab R2020a on macOS</strong></p>

<p>Add matlab executable to the PATH variable</p>

<p><code>export PATH=&quot;/Applications/MATLAB_R2020a.app/bin:$PATH&quot;</code></p>

<p>Launch in nodesktop mode:</p>

<p><code>matlab -nodesktop</code></p>

<p>Within Matlab Console</p>

<p><code>createModel(&quot;DiLiGenT/bearPNG&quot;, &#39;bear.stl&#39;, 7)</code></p>

<p><code>renderMV(&quot;DiLiGenT-MV/buddhaPNG&quot;)</code></p>
</blockquote>

<p><code>loadData</code>: loads images and spherical coordinates from one of the directories in any of the DiLiGenT datasets (example: loadData(“DiLiGenT/bearPNG”)). The expectation is that there is a filenames.txt file with the names of all image files listed, and a light_directions.txt file with all the light directions in cartesian coordinates listed if you want to use pre-measured light directions.</p>

<p><code>createTrainValidation</code>: splits the DiLiGenT dataset into training and validation data for training the convolutional neural network</p>

<p><code>trainLightDir</code>: trains a CNN to predict the spherical coordinates of the light direction in a single image</p>

<p><code>getLightDir</code>: translates all spherical coordinates in dataset to cartesian coordinates</p>

<p><code>getNormals</code>: uses images and their corresponding light directions to create a normal map</p>

<p><code>kMeansCluster</code>: clusters pixels in normal map according to coordinate and surface normal and averages the surface normals in each cluster</p>

<p><code>normal2depth</code>: calculates a depth map from a normal map using a naive approach (very noisy)</p>

<p><code>frankotChellappa</code>: This routine can be found in <a href="https://github.com/dhr/matlab-tools/blob/master/ptgf/frankotChellappa.m">matlab-tools</a> repository. It creates a depth map from a normal map using the Frankot-Chellappa algorithm.</p>

<p><code>renderModel</code>: renders a 3d model from a directory in the DiLiGenT dataset. The first parameter specifies the input directory and the second specifies the number of clusters with a value of 0 skipping the clustering (example: renderModel(“DiLiGenT/bearPNG”, 7)). This function assumes the provided directory follows the same restrictions specified in the loadData function, and it must also include a mask.png to only include the object without the background.</p>

<p><code>createModel</code>: creates an stl file from a directory in the DiLiGenT dataset. The first parameter specifies the directory, the second specifies the filename for the output, and the third specifies the number of clusters with a value of 0 skipping the clustering (example: createModel(“DiLiGenT/bearPNG”, ‘bear.stl’, 7)). This function has the same assumptions for the directory as the renderModel function.</p>

<p>Besides the functions we added to the GitHub repo, there are some built-in functions to use. These include: <code>load(“light_cnn_inceptionv3”)</code> to load a pre-trained network, <code>predict(net, imgs)</code> for using CNN net with input imgs to predict the light source directions, and the matlab add-on <code>surf2stl</code> for turning a depthmap into an stl file.</p>

        </div>
        <div class="dark-box">
            <div class="lang-selector">
                  <a href="#" data-language-name="matlab">---</a>
            </div>
        </div>
      </div>
    </div>

    <script src="https://d3js.org/d3.v3.min.js"></script>
    <script src="https://cdn.rawgit.com/riccardoscalco/textures/master/dist/textures.js"></script>
    <script src="javascripts/header-06fc3421.js"></script>
  </body>
</html>
